{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CfO5jDQ3iHX"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_XMuQjN2-jp",
        "outputId": "edebdbf3-0f6f-46a8-d428-22ec14f3baa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "import seaborn as sns\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ymcSmUeNp30s"
      },
      "outputs": [],
      "source": [
        "# Utilised functions\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, 'wb') as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = '1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1'\n",
        "    toy_url = 'https://docs.google.com/uc?export=download'\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print('Downloading FEVER data splits...')\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print('Download completed!')\n",
        "\n",
        "        print('Extracting dataset...')\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print('Extraction completed!')\n",
        "\n",
        "remove_dict = [', links = no', '-lrb-', '(', '-rrb-', ')', '-lsb-', '[', '-rsb-', ']', '`', '`', '|', '@', ',', ';', ':', '``', '\\'\\'', 'langpron']\n",
        "\n",
        "def text_prepare_tokenize(sentence):\n",
        "    # Only take sentence, remove ID and keywords\n",
        "    if '\\t' in sentence:\n",
        "      sentence = sentence.split('\\t')[1]\n",
        "\n",
        "    # Transforms given text to lower case\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Remove special characters with space\n",
        "    for key in remove_dict:\n",
        "        sentence = sentence.replace(key, '')\n",
        "\n",
        "    # Removes any special character that is not in the good symbols list\n",
        "    sentence = (re.compile('[^0-9a-z. \\t]')).sub('', sentence)\n",
        "      \n",
        "    # Removes any left or right spacing\n",
        "    sentence.strip()\n",
        "\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "def encode(sentence):\n",
        "    encoding = []\n",
        "    for word in sentence:\n",
        "        if word in word_dict:\n",
        "            encoding.append(word_dict[word])\n",
        "        else:\n",
        "            encoding.append(word_dict['OOV'])\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XD2zcELx3F6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c1e91c-adac-4256-d81f-3d6bde654565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading FEVER data splits...\n",
            "Download completed!\n",
            "Extracting dataset...\n",
            "Extraction completed!\n"
          ]
        }
      ],
      "source": [
        "# Download and unzip the dataset\n",
        "download_data('dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YWOrLkpj0mVF"
      },
      "outputs": [],
      "source": [
        "# Load the datasets and filter out unwanted columns\n",
        "train_data = pd.read_csv('/content/dataset/train_pairs.csv')\n",
        "train_data = train_data[['Claim', 'Evidence', 'Label', 'ID']]\n",
        "\n",
        "val_data = pd.read_csv('/content/dataset/val_pairs.csv')\n",
        "val_data = val_data[['Claim', 'Evidence', 'Label', 'ID']]\n",
        "\n",
        "test_data = pd.read_csv('/content/dataset/test_pairs.csv')\n",
        "test_data = test_data[['Claim', 'Evidence', 'Label', 'ID']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "collapsed": true,
        "id": "6hxHvqUX2GB8",
        "outputId": "9cd4075f-5a87-41b7-f82d-78f2a5a41bff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Breakdown of training data\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "sns.set_theme(style = 'darkgrid')\n",
        "sns.countplot(x = 'Label', data = train_data, palette='Set3')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlAidmQp6ZL_",
        "outputId": "f4dd64aa-cc18-4f86-a156-b42eb7d258e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 121740/121740 [00:25<00:00, 4751.39it/s]\n",
            "100%|██████████| 121740/121740 [00:25<00:00, 4728.67it/s]\n",
            "100%|██████████| 7165/7165 [00:00<00:00, 7672.95it/s]\n",
            "100%|██████████| 7165/7165 [00:01<00:00, 5069.50it/s]\n",
            "100%|██████████| 7189/7189 [00:00<00:00, 7912.30it/s]\n",
            "100%|██████████| 7189/7189 [00:01<00:00, 5004.21it/s]\n"
          ]
        }
      ],
      "source": [
        "# Tokenise the claims and evidence of the datasets\n",
        "train_data['Claim'] = train_data['Claim'].progress_apply(lambda txt: text_prepare_tokenize(txt))\n",
        "train_data['Evidence'] = train_data['Evidence'].progress_apply(lambda txt: text_prepare_tokenize(txt))\n",
        "\n",
        "val_data['Claim'] = val_data['Claim'].progress_apply(lambda txt: text_prepare_tokenize(txt))\n",
        "val_data['Evidence'] = val_data['Evidence'].progress_apply(lambda txt: text_prepare_tokenize(txt))\n",
        "\n",
        "test_data['Claim'] = test_data['Claim'].progress_apply(lambda txt: text_prepare_tokenize(txt))\n",
        "test_data['Evidence'] = test_data['Evidence'].progress_apply(lambda txt: text_prepare_tokenize(txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaFuBWtiTaXG",
        "outputId": "f140317c-ae85-4cad-97f7-cd35b8578631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 35980\n"
          ]
        }
      ],
      "source": [
        "# Build the word dictionary using the training data\n",
        "word_dict = {}\n",
        "word_lst = [np.hstack(train_data['Claim'].values), np.hstack(train_data['Evidence'].values)]\n",
        "words = np.hstack(word_lst)\n",
        "\n",
        "# Start from 1 as 0 will be used for encoding the padding\n",
        "i = 1\n",
        "for word in words:\n",
        "    if word not in word_dict:\n",
        "        word_dict[word] = i\n",
        "        i += 1\n",
        "\n",
        "# Add an out-of-vocabulary encoding for words not in the training data\n",
        "word_dict['OOV'] = len(word_dict)+1\n",
        "\n",
        "print(f'Number of unique words: {len(word_dict)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jDsi3rLTaAKD"
      },
      "outputs": [],
      "source": [
        "# Encode the claims and evidence of the datasets\n",
        "train_data['Claim_Tokens'] = train_data['Claim'].apply(encode)\n",
        "train_data['Evidence_Tokens'] = train_data['Evidence'].apply(encode)\n",
        "\n",
        "val_data['Claim_Tokens'] = val_data['Claim'].apply(encode)\n",
        "val_data['Evidence_Tokens'] = val_data['Evidence'].apply(encode)\n",
        "\n",
        "test_data['Claim_Tokens'] = test_data['Claim'].apply(encode)\n",
        "test_data['Evidence_Tokens'] = test_data['Evidence'].apply(encode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0OLRRN3Pg5l",
        "outputId": "357081f9-d9f8-403d-a2d7-9af02a473979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length of the sequence: 100\n"
          ]
        }
      ],
      "source": [
        "# Pad the sequences to the length of the longest sequence with zeros\n",
        "max_length = 100\n",
        "print(f'Maximum length of the sequence: {max_length}')\n",
        "\n",
        "train_data['Claim_Tokens'] = list(pad_sequences(train_data['Claim_Tokens'], maxlen = max_length, padding='post'))\n",
        "train_data['Evidence_Tokens'] = list(pad_sequences(train_data['Evidence_Tokens'], maxlen = max_length, padding='post'))\n",
        "\n",
        "val_data['Claim_Tokens'] = list(pad_sequences(val_data['Claim_Tokens'], maxlen = max_length, padding='post'))\n",
        "val_data['Evidence_Tokens'] = list(pad_sequences(val_data['Evidence_Tokens'], maxlen = max_length, padding='post'))\n",
        "\n",
        "test_data['Claim_Tokens'] = list(pad_sequences(test_data['Claim_Tokens'], maxlen = max_length, padding='post'))\n",
        "test_data['Evidence_Tokens'] = list(pad_sequences(test_data['Evidence_Tokens'], maxlen = max_length, padding='post'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "SEBmklDIAyDY",
        "outputId": "29ff3f78-7a2c-4d39-9908-35c2ddea7cbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2ea74545-6ce7-4243-ab92-ad18b1713bd8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Claim</th>\n",
              "      <th>Claim_Tokens</th>\n",
              "      <th>Evidence</th>\n",
              "      <th>Evidence_Tokens</th>\n",
              "      <th>Label</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[chris, hemsworth, appeared, in, a, perfect, g...</td>\n",
              "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[hemsworth, has, also, appeared, in, the, scie...</td>\n",
              "      <td>[2, 15, 533, 3, 4, 46, 2871, 3072, 2100, 94, 7...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[roald, dahl, is, a, writer, .]</td>\n",
              "      <td>[9, 10, 11, 5, 12, 8, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[roald, dahl, ro.ld, dl, ul, dl, 13, september...</td>\n",
              "      <td>[9, 10, 20294, 20295, 20296, 20295, 323, 758, ...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[roald, dahl, is, a, governor, .]</td>\n",
              "      <td>[9, 10, 11, 5, 13, 8, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[roald, dahl, ro.ld, dl, ul, dl, 13, september...</td>\n",
              "      <td>[9, 10, 20294, 20295, 20296, 20295, 323, 758, ...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[ireland, has, relatively, lowlying, mountains...</td>\n",
              "      <td>[14, 15, 16, 17, 18, 8, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[the, island, s, geography, comprises, relativ...</td>\n",
              "      <td>[46, 447, 8767, 10171, 3733, 16, 17, 18, 11003...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ireland, does, not, have, relatively, lowlyin...</td>\n",
              "      <td>[14, 19, 20, 21, 16, 17, 18, 8, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>[the, island, s, geography, comprises, relativ...</td>\n",
              "      <td>[46, 447, 8767, 10171, 3733, 16, 17, 18, 11003...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ea74545-6ce7-4243-ab92-ad18b1713bd8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2ea74545-6ce7-4243-ab92-ad18b1713bd8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2ea74545-6ce7-4243-ab92-ad18b1713bd8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               Claim  ...  ID\n",
              "0  [chris, hemsworth, appeared, in, a, perfect, g...  ...   3\n",
              "1                    [roald, dahl, is, a, writer, .]  ...   7\n",
              "2                  [roald, dahl, is, a, governor, .]  ...   8\n",
              "3  [ireland, has, relatively, lowlying, mountains...  ...   9\n",
              "4  [ireland, does, not, have, relatively, lowlyin...  ...  10\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Preview the training dataset\n",
        "train_data = train_data[['Claim', 'Claim_Tokens', 'Evidence', 'Evidence_Tokens', 'Label', 'ID']]\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0AY0ERJmBTsE"
      },
      "outputs": [],
      "source": [
        "# Encode the labels as binary values\n",
        "train_labels = np.array([1 if i == 'SUPPORTS' else 0 for i in train_data['Label']], dtype=np.int32)\n",
        "val_labels = np.array([1 if i == 'SUPPORTS' else 0 for i in val_data['Label']], dtype=np.int32)\n",
        "test_labels = np.array([1 if i == 'SUPPORTS' else 0 for i in test_data['Label']], dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XcoO2p_ciqQ_"
      },
      "outputs": [],
      "source": [
        "# Pair up the claim and evidence tokens for training\n",
        "train_tokens = [np.array(train_data['Claim_Tokens'].to_list(), dtype = np.int32), np.array(train_data['Evidence_Tokens'].to_list(), dtype = np.int32)]\n",
        "val_tokens = [np.array(val_data['Claim_Tokens'].to_list(), dtype = np.int32), np.array(val_data['Evidence_Tokens'].to_list(), dtype = np.int32)]\n",
        "test_tokens = [np.array(test_data['Claim_Tokens'].to_list(), dtype = np.int32), np.array(test_data['Evidence_Tokens'].to_list(), dtype = np.int32)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Qc6TAXiWu4"
      },
      "source": [
        "# Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yaUAbgbNie6_"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import gensim.downloader as gloader\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Dropout, Embedding, Concatenate, Add, Average, Dot, Bidirectional, Multiply, Reshape, Flatten, GlobalAveragePooling1D\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from keras import metrics\n",
        "from keras.backend import reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VuvM48i8id9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "540547c5-bf39-4019-f175-91da4dac49b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "(35981, 50)\n"
          ]
        }
      ],
      "source": [
        "# Download the GloVe embeddings and build an embedding matrix\n",
        "embedding_dimension = 50\n",
        "embedding_model = gloader.load(\"glove-wiki-gigaword-{}\".format(embedding_dimension))\n",
        "\n",
        "embedding_matrix =  [np.zeros((embedding_dimension))]\n",
        "for word in word_dict.keys() :\n",
        "    if word in embedding_model.vocab :\n",
        "        embedding_matrix.append(embedding_model[word])\n",
        "    else:\n",
        "        embedding_matrix.append(np.random.uniform(low = -0.05, high = 0.05, size = embedding_dimension))\n",
        "embedding_matrix = np.array(embedding_matrix)\n",
        "\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l7WUVonDxQ8i"
      },
      "outputs": [],
      "source": [
        "# Definition of the neural network architecture\n",
        "def create_model(embedding_matrix, chosen_sentence_embedding = 1, chosen_merge = 1, chosen_cosine = True):\n",
        "\n",
        "    # Split the input as claim and evidence\n",
        "    input_claim = Input(shape = max_length, dtype=np.int32)\n",
        "    input_evidence = Input(shape = max_length, dtype=np.int32)\n",
        "\n",
        "    # Embedding layer\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim = len(word_dict) + 1,\n",
        "        output_dim = 50,\n",
        "        weights = [embedding_matrix],\n",
        "        input_length=100,\n",
        "        mask_zero = True,\n",
        "        trainable = False\n",
        "    )\n",
        "    claim_embedding = embedding_layer(input_claim)\n",
        "    evidence_embedding = embedding_layer(input_evidence)\n",
        "\n",
        "    # Sentence embedding methods\n",
        "    # Final state of RNN\n",
        "    if chosen_sentence_embedding == 1:\n",
        "        _, forward_h, _, backward_h, _ = Bidirectional(LSTM(128, dropout = 0.2, return_state = True))(claim_embedding)\n",
        "        claim_sentence_embedding = Concatenate()([forward_h, backward_h])\n",
        "        _, forward_h, _, backward_h, _ = Bidirectional(LSTM(128, dropout = 0.2, return_state = True))(evidence_embedding)\n",
        "        evidence_sentence_embedding = Concatenate()([forward_h, backward_h])\n",
        "    \n",
        "    # Average states of RNN\n",
        "    elif chosen_sentence_embedding == 2:\n",
        "        claim_sentence_embedding = GlobalAveragePooling1D()(Bidirectional(LSTM(128, return_sequences = True))(claim_embedding))\n",
        "        evidence_sentence_embedding = GlobalAveragePooling1D()(Bidirectional(LSTM(128, return_sequences = True))(evidence_embedding))\n",
        "    \n",
        "    # MLP\n",
        "    elif chosen_sentence_embedding == 3:\n",
        "        claim_embedding = Flatten()(claim_embedding)\n",
        "        claim_sentence_embedding = Dense(128)(claim_embedding)\n",
        "        evidence_embedding = Flatten()(evidence_embedding)\n",
        "        evidence_sentence_embedding = Dense(128)(evidence_embedding)\n",
        "    \n",
        "    # Average of word embeddings\n",
        "    elif chosen_sentence_embedding == 4:\n",
        "        claim_sentence_embedding = GlobalAveragePooling1D()(claim_embedding)\n",
        "        evidence_sentence_embedding = GlobalAveragePooling1D()(evidence_embedding)\n",
        "\n",
        "    # Embedding merging methods\n",
        "    # Concatenate the embeddings\n",
        "    if chosen_merge == 1:\n",
        "        merged_input = Concatenate()([claim_sentence_embedding, evidence_sentence_embedding])\n",
        "    \n",
        "    # Sum the embeddings\n",
        "    elif chosen_merge == 2:\n",
        "        merged_input = Add()([claim_sentence_embedding, evidence_sentence_embedding])\n",
        "    \n",
        "    # Average the embeddings\n",
        "    elif chosen_merge == 3:\n",
        "        merged_input = Average()([claim_sentence_embedding, evidence_sentence_embedding])\n",
        "    \n",
        "    # Concatenate the cosine similarities\n",
        "    if chosen_cosine:\n",
        "        cosine_similarity = Dot(axes = 1, normalize = True)([claim_sentence_embedding, evidence_sentence_embedding])\n",
        "        merged_input = Concatenate()([merged_input, cosine_similarity])\n",
        "\n",
        "    # Model architecture\n",
        "    layer = Dense(256, input_dim = 20, activation = 'relu', kernel_regularizer=regularizers.l2(1e-4))(merged_input)\n",
        "    dropout = Dropout(0.2)(layer)\n",
        "    layer = Dense(64, activation = 'relu', kernel_regularizer=regularizers.l2(1e-4))(dropout)\n",
        "    dropout = Dropout(0.2)(layer)\n",
        "    out = Dense(1, activation = \"sigmoid\")(dropout)\n",
        "\n",
        "    return Model(inputs = [input_claim, input_evidence], outputs = [out])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "y4VUxur_A6rQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4UkWY8WwiuCM"
      },
      "outputs": [],
      "source": [
        "# # Train the defined model\n",
        "# model = create_model(embedding_matrix, chosen_sentence_embedding = 2, chosen_merge = 1, chosen_cosine = True)\n",
        "\n",
        "# model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 1e-3), metrics = [metrics.AUC()])\n",
        "\n",
        "# history = model.fit(x = train_tokens, \n",
        "#                      y = train_labels, \n",
        "#                      batch_size = 128,\n",
        "#                      epochs = 5, \n",
        "#                      callbacks = EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True),\n",
        "#                      validation_data = (val_tokens, val_labels))\n",
        "\n",
        "# # Save the trained model\n",
        "# model.save('models/model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnYmDacxbES8"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icib90P1Zbm3",
        "outputId": "310c01ab-0d47-43ee-a28c-f2596ebe3d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1t1NOS6SQ1tm51AvmJXZexFTlQxirSMrs\n",
            "To: /content/models.zip\n",
            "100% 88.5M/88.5M [00:01<00:00, 46.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the trained models\n",
        "!gdown --id 1t1NOS6SQ1tm51AvmJXZexFTlQxirSMrs\n",
        "\n",
        "# Unzip the trained models\n",
        "!unzip -q -o models.zip -d models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kQ9TMv6GD2_l"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import classification_report\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fXA-LFdJZfFX"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "best_model = load_model('/content/models/models/model_21t.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ykag-kioFBj1"
      },
      "outputs": [],
      "source": [
        "# Get the predictions of the best model using the test tokens\n",
        "pred_labels = best_model.predict(test_tokens)\n",
        "pred_labels = np.rint(pred_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJbmMLaLbKdi",
        "outputId": "50addedb-d619-4bd3-9952-61670cd1a27a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.90      0.50      0.64      3583\n",
            "    SUPPORTS       0.66      0.95      0.77      3606\n",
            "\n",
            "    accuracy                           0.72      7189\n",
            "   macro avg       0.78      0.72      0.71      7189\n",
            "weighted avg       0.78      0.72      0.71      7189\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Multi-input classification evaluation\n",
        "print(classification_report(test_labels, pred_labels, zero_division = 1, target_names = [\"REFUTES\", \"SUPPORTS\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3G0zVysdBjO",
        "outputId": "57144cff-e759-4141-cd0f-dc403707a82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.90      0.51      0.65      3304\n",
            "    SUPPORTS       0.66      0.95      0.78      3309\n",
            "\n",
            "    accuracy                           0.73      6613\n",
            "   macro avg       0.78      0.73      0.71      6613\n",
            "weighted avg       0.78      0.73      0.71      6613\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Claim verification evaluation\n",
        "majority_voting = pd.DataFrame(columns = ['ID', 'True', 'Pred'])\n",
        "majority_voting['ID'] = pd.unique(test_data['ID'])\n",
        "\n",
        "predictions =  pd.DataFrame(columns = ['ID', 'Label'])\n",
        "predictions[\"Label\"] = [item for sublist in pred_labels for item in sublist]\n",
        "predictions['ID'] = test_data['ID']\n",
        "\n",
        "for id in majority_voting['ID']:\n",
        "  \n",
        "    # Get the true labels\n",
        "    majority_voting['True'][majority_voting['ID'] == id] = test_data['Label'][test_data['ID'] == id].iloc[0]\n",
        "\n",
        "    # Get all the predicted label given an id/claim\n",
        "    labels = predictions[predictions['ID'] == id]\n",
        "\n",
        "    # Implement majority vote\n",
        "    if(len(labels[labels['Label'] == 1]) > len(labels[labels['Label'] == 0])):  \n",
        "        majority_voting['Pred'][majority_voting['ID'] == id] = 'SUPPORTS'\n",
        "    else:\n",
        "        majority_voting['Pred'][majority_voting['ID'] == id] = 'REFUTES'\n",
        "\n",
        "print(classification_report(majority_voting['True'], majority_voting['Pred'], zero_division = 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comments"
      ],
      "metadata": {
        "id": "-q7oES3BYtaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, a fact checking task was solved by training a classifier on a set of sentence embeddings.  The claims and their evidence are first cleaned and encoded as sentence embeddings. Finally, a classifier is trained on these embeddings to predict if a claim is supported or rejected.  Results show that the best approach achieves an AUC of 85% and F1-score of 71%."
      ],
      "metadata": {
        "id": "ISJpviGAZ6Us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model uploaded for the evaluation is the best model, but it can be changed.\n",
        "Not all model are pre-trained and they are named in the following way: *model_abc*\n",
        "\n",
        "a\n",
        "*  1 -> RNN final state\n",
        "*  2 -> RNN average states\n",
        "*  3 -> Multilayer Perceptron\n",
        "*  4 -> Bag of Vectors\n",
        "\n",
        "b\n",
        "*  1 -> Concatenation\n",
        "*  2 -> Sum\n",
        "*  3 -> Mean\n",
        "\n",
        "c\n",
        "* t -> cosine similarity concateneted\n",
        "* f -> cosine similarity not concateneted"
      ],
      "metadata": {
        "id": "8Qb6VmDqXQK7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}